import argparse
import asyncio
import requests
from pathlib import Path
import json
from typing import List, Dict

from utils import load_utils
from utils.prompt_utils import PromptUtils
from utils.llm_client import QWQ_LLM, RequestType
from utils.logger_utils import LoggerUtil

logger = LoggerUtil.get_logger("F1_Evaluator")


class F1_Evaluator:
    def __init__(self, optimized_path: str = "",
                 model_url: str = "",
                 dataset_name: str = "",
                 dataset_path: str = "",
                 api_key: str = ""):
        logger.info(f"åˆå§‹åŒ– F1_Evaluator")
        self.root_path = Path(optimized_path)
        self.prompt_utils = PromptUtils(self.root_path)
        # self.model_url = model_url  # LLM çš„ API åœ°å€
        # self.dataset_name = dataset_name.lower()
        # self.dataset_path = dataset_path
        # self.api_key = api_key
        # self.api_type = "openai"
        # self.data_utils = DataUtils(self.root_path)
        # QWQ_LLM.initialize(
        #     optimize_kwargs={"use_ollama": False},
        #     evaluate_kwargs={"use_ollama": False},
        #     execute_kwargs={"use_ollama": False},
        #     analyze_kwargs={"use_ollama": False},
        #     generate_kwargs={"use_ollama": False}
        # )
        # self.llm_client = QWQ_LLM.get_instance()

    async def query_llm_async(self, prompt: str, question: str) -> str:
        """å¼‚æ­¥è°ƒç”¨ QWQ_LLM å®¢æˆ·ç«¯æ¥å£è·å–å›ç­”"""
        full_prompt = f"{prompt}\n\n{question}"
        logger.info(f"æŸ¥è¯¢ LLMï¼Œé—®é¢˜: {question}")
        try:
            # è¿™é‡Œæˆ‘ä»¬é€‰æ‹© EXECUTE ç±»å‹è°ƒç”¨ï¼Œå¯æ ¹æ®å®é™…åœºæ™¯è°ƒæ•´ RequestType
            answer = await self.llm_client.responser(request_type=RequestType.EXECUTE,
                                                     messages=[{"role": "user", "content": full_prompt}])
            logger.info(f"LLM è¿”å›ç­”æ¡ˆ: {answer}")
            return answer
        except Exception as e:
            logger.error(f"è°ƒç”¨ LLM å‡ºç°å¼‚å¸¸: {e}")
            return ""

    def query_llm(self, prompt: str, question: str) -> str:
        """å¯¹å¤–æä¾›åŒæ­¥æ¥å£ï¼Œå†…éƒ¨è°ƒç”¨å¼‚æ­¥ query_llm_async"""
        return asyncio.run(self.query_llm_async(prompt, question))

    # def load_data(self) -> List[Dict]:
    #     """
    #     åŠ è½½æ•°æ®é›†ï¼Œæ ¹æ® dataset_name åˆ¤æ–­åŠ è½½æ–¹å¼ï¼š
    #       - å¦‚æœæ˜¯ "bbh-navigate" æˆ– "bigbench"ï¼Œåˆ™ä½¿ç”¨ Hugging Face datasets åŠ è½½ BigBench çš„ navigate å­é›†ï¼›
    #       - å¦‚æœæ˜¯ "liar"ã€"wsc" æˆ– "avg.perf."ï¼ˆåŠå…¶å˜ä½“ï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ load_dataset å¯¼å…¥ï¼›
    #       - å¦‚æœæ˜¯ "gpqa"ï¼Œåˆ™ä»æœ¬åœ° JSON æ–‡ä»¶åŠ è½½ï¼›
    #       - å…¶ä»–æƒ…å†µï¼Œè¾“å‡ºä¸æ”¯æŒçš„æç¤ºã€‚
    #     è¿”å›åŠ è½½çš„æ ·æœ¬æ•°æ®
    #     """
    #     logger.info(f"åŠ è½½æ•°æ®é›†: {self.dataset_name}")
    #     if self.dataset_name in ["bbh-navigate", "bigbench"]:
    #         dataset = load_dataset("bigbench", "navigate")
    #         logger.info(f"æˆåŠŸåŠ è½½ BigBench 'navigate' å­é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
    #         return dataset["test"]
    #     elif self.dataset_name == "liar":
    #         dataset = load_dataset("liar")
    #         logger.info(f"æˆåŠŸåŠ è½½ LIAR æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
    #         return dataset["test"]
    #     elif self.dataset_name == "wsc":
    #         dataset = load_dataset("wsc")
    #         logger.info(f"æˆåŠŸåŠ è½½ WSC æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
    #         return dataset["test"]
    #     elif self.dataset_name in ["avg.perf.", "avg_perf", "avgperf"]:
    #         dataset = load_dataset("avg_perf")
    #         logger.info(f"æˆåŠŸåŠ è½½ Avg.Perf. æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
    #         return dataset["test"]
    #     elif self.dataset_name == "gpqa":
    #         dataset_file = Path(self.dataset_path)
    #         if not dataset_file.exists():
    #             logger.error(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")
    #             return []
    #         try:
    #             with open(dataset_file, 'r', encoding='utf-8') as f:
    #                 data = json.load(f)
    #             logger.info(f"æˆåŠŸåŠ è½½ GPQA æ•°æ®é›†ï¼Œå…± {len(data)} æ¡æ•°æ®")
    #             return data
    #         except json.JSONDecodeError as e:
    #             logger.error(f"è§£æ JSON å¤±è´¥: {e}")
    #             return []
    #     else:
    #         logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {self.dataset_name}")
    #         return []

    def calculate_f1(self, prediction: str, ground_truth: str):
        """è®¡ç®—å•ä¸ª F1 åˆ†æ•°"""
        pred_tokens = prediction.split()
        truth_tokens = ground_truth.split()
        common = set(pred_tokens) & set(truth_tokens)

        if not common:
            return 0.0

        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(truth_tokens)
        f1 = 2 * (precision * recall) / (precision + recall)
        logger.info(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
        return f1

    def calculate_f1_list(self, data: List[Dict[str, str]], predictions: List[str]) -> float:
        """è®¡ç®—æ•°æ®é›†çš„å¹³å‡ F1 åˆ†æ•°"""
        required_keys = {"question", "answer"}
        f1_scores = []

        if len(data) != len(predictions):
            raise ValueError("data å’Œ predictions é•¿åº¦ä¸ä¸€è‡´")
        for item, prediction in zip(data, predictions):
            if not required_keys.issubset(item.keys()):
                logger.error(f"æ•°æ®å­—æ®µç¼ºå¤±ï¼š{item}")

            question = item.get("question")
            ground_truth = item.get("answer")

            if not question or not ground_truth:
                logger.warning("æ ·æœ¬æ•°æ®ç¼ºå°‘ question æˆ– answer å­—æ®µï¼Œè·³è¿‡è¯¥æ ·æœ¬ã€‚")
                continue

            # logger.info(f"ğŸ” é—®é¢˜: {question}, ç­”æ¡ˆ: {ground_truth}, é¢„æµ‹: {prediction}")

            f1 = self.calculate_f1(prediction, ground_truth)
            f1_scores.append(f1)

        avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
        logger.info(f"ğŸ“Š æ•°æ®é›†å¹³å‡ F1 åˆ†æ•°: {avg_f1:.4f}")
        return avg_f1

    def execute(self, prompt: str, qa: List[Dict]) -> List[Dict]:
        """æ‰§è¡ŒllmæŸ¥è¯¢"""
        logger.info(f"å¼€å§‹å¤„ç† {len(qa)} æ¡æ•°æ®")
        results = []
        for item in qa:
            question = item.get("question")
            answer = self.query_llm(prompt, question)
            results.append({"question": question, "answer": answer})
        return results

    def evaluate_spo(self):
        """è®¡ç®—F1åˆ†æ•°"""
        logger.info("å¼€å§‹è¯„ä¼°")
        prompt = self.prompt_utils.get_final_prompt()
        data = load_utils.load_meta_data()
        if not data:
            logger.error("æœªåŠ è½½åˆ°æ•°æ®ï¼Œè¯„ä¼°ç»ˆæ­¢ã€‚")
            return 0.0

        answers = self.execute(prompt, data)
        avg_f1 = self.calculate_f1_list(data, answers)
        logger.info(f"ğŸ“Š æ•°æ®é›†å¹³å‡ F1 åˆ†æ•°: {avg_f1:.4f}")
        return avg_f1


def parse_args():
    parser = argparse.ArgumentParser(description="SPO PromptOptimizer CLI")
    parser.add_argument("--uid", type=str, default="3991ad42-c46b-4f2f-9dde-de015aaf5bde", help="ä¼˜åŒ–è¾“å‡ºè·¯å¾„çš„ UID")
    parser.add_argument("--name", type=str, default="Navigate", help="é¡¹ç›®åç§°")
    parser.add_argument("--model-url", type=str, default="https://api.chatanywhere.com.cn/v1", help="LLM æ¨¡å‹æ¥å£åœ°å€")
    parser.add_argument("--api-key", type=str, default="sk-iX0M9keAJemCgNFqvQMVLyWkcembRT27ix50aymLnvZ18QuT",
                        help="çº¿ä¸Š API çš„å¯†é’¥")
    parser.add_argument("--dataset-name", type=str, default="bigbench", help="æ•°æ®é›†åç§°")
    parser.add_argument("--dataset-path", type=str, default="dataset", help="æœ¬åœ°æ•°æ®é›†è·¯å¾„")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    evaluator = F1_Evaluator(
        optimized_path=str(Path("workspace") / args.uid / args.name),
        model_url=args.model_url,
        dataset_name=args.dataset_name,
        dataset_path=args.dataset_path,
        api_key=args.api_key,
    )
    # evaluator.evaluate_spo()
