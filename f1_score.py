import argparse
import requests
from pathlib import Path
import json
from typing import List, Dict
from datasets import load_dataset

from utils.data_utils import DataUtils

from utils.logger_util import LoggerUtil
logger=LoggerUtil.get_logger("F1_Evaluator")

class F1_Evaluator:
    # å¯åœ¨å…¶ä»–ç±»ä¸­è°ƒç”¨ F1_Evaluator.evaluateè®¡ç®—è¾“å‡ºåçš„F1åˆ†æ•°
    def __init__(self, optimized_path: str,
                 model_url: str,
                 dataset_name: str,
                 dataset_path: str,
                 api_key: str = "",
                 ):
        logger.info(f"åˆå§‹åŒ– F1_Evaluatorï¼Œä¼˜åŒ–è·¯å¾„: {optimized_path}")
        self.root_path = Path(optimized_path)
        self.data_utils = DataUtils(self.root_path)
        self.model_url = model_url  # LLM çš„ API åœ°å€
        self.dataset_name = dataset_name.lower()
        self.dataset_path = dataset_path
        self.api_key = api_key
        self.api_type = "openai"

    def get_final_prompt(self):
        """è·å–æœ€åä¸€è½®è¿­ä»£ä¼˜åŒ–å‡ºçš„ prompt"""
        best_round = self.data_utils.get_best_round()
        if not best_round:
            logger.error("æ— æ³•æ‰¾åˆ°æœ€ä½³ promptï¼Œé»˜è®¤è¿”å›ç©ºå­—ç¬¦ä¸²")
            return ""
        logger.info(f"åŠ è½½åˆ°æœ€ä½³ prompt: {best_round['prompt']}")
        return best_round["prompt"]

    def load_data(self) -> List[Dict]:
        """
        åŠ è½½æ•°æ®é›†ï¼Œæ ¹æ® dataset_name åˆ¤æ–­åŠ è½½æ–¹å¼ï¼š
          - å¦‚æœæ˜¯ "bbh-navigate" æˆ– "bigbench"ï¼Œåˆ™ä½¿ç”¨ Hugging Face datasets åŠ è½½ BigBench çš„ navigate å­é›†ï¼›
          - å¦‚æœæ˜¯ "liar"ã€"wsc" æˆ– "avg.perf."ï¼ˆåŠå…¶å˜ä½“ï¼‰ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ load_dataset å¯¼å…¥ï¼›
          - å¦‚æœæ˜¯ "gpqa"ï¼Œåˆ™ä»æœ¬åœ° JSON æ–‡ä»¶åŠ è½½ï¼›
          - å…¶ä»–æƒ…å†µï¼Œè¾“å‡ºä¸æ”¯æŒçš„æç¤ºã€‚
        è¿”å›åŠ è½½çš„æ ·æœ¬æ•°æ®
        """
        logger.info(f"åŠ è½½æ•°æ®é›†: {self.dataset_name}")
        if self.dataset_name in ["bbh-navigate", "bigbench"]:
            dataset = load_dataset("bigbench", "navigate")
            logger.info(f"æˆåŠŸåŠ è½½ BigBench 'navigate' å­é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
            return dataset["test"]
        elif self.dataset_name == "liar":
            dataset = load_dataset("liar")
            logger.info(f"æˆåŠŸåŠ è½½ LIAR æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
            return dataset["test"]
        elif self.dataset_name == "wsc":
            dataset = load_dataset("wsc")
            logger.info(f"æˆåŠŸåŠ è½½ WSC æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
            return dataset["test"]
        elif self.dataset_name in ["avg.perf.", "avg_perf", "avgperf"]:
            dataset = load_dataset("avg_perf")
            logger.info(f"æˆåŠŸåŠ è½½ Avg.Perf. æ•°æ®é›†ï¼Œå…± {len(dataset['test'])} æ¡æ•°æ®")
            return dataset["test"]
        elif self.dataset_name == "gpqa":
            dataset_file = Path(self.dataset_path)
            if not dataset_file.exists():
                logger.error(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")
                return []
            try:
                with open(dataset_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                logger.info(f"æˆåŠŸåŠ è½½ GPQA æ•°æ®é›†ï¼Œå…± {len(data)} æ¡æ•°æ®")
                return data
            except json.JSONDecodeError as e:
                logger.error(f"è§£æ JSON å¤±è´¥: {e}")
                return []
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {self.dataset_name}")
            return []
    def query_llm(self, prompt: str, question: str):
        """åŒæ­¥è°ƒç”¨çº¿ä¸Š LLM æŸ¥è¯¢æ¥å£"""
        logger.info(f"æŸ¥è¯¢ LLM: {self.model_url}ï¼Œé—®é¢˜: {question}")
        payload = {"prompt": f"{prompt}\n\n{question}", "max_tokens": 256}
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Api-Type": self.api_type,
            "Content-Type": "application/json"
        }
        try:
            response = requests.post(self.model_url, json=payload, headers=headers)
            response.raise_for_status()
            answer = response.json().get("text", "").strip()
            logger.info(f"LLM è¿”å›ç­”æ¡ˆ: {answer}")
            return answer
        except requests.RequestException as e:
            logger.error(f"è¯·æ±‚çº¿ä¸Š API å¤±è´¥: {e}")
            return ""

    def compute_f1(self, prediction: str, ground_truth: str):
        """è®¡ç®— F1 åˆ†æ•°"""
        logger.info(f"è®¡ç®— F1ï¼Œé¢„æµ‹: {prediction}ï¼Œæ ‡å‡†ç­”æ¡ˆ: {ground_truth}")
        pred_tokens = prediction.split()
        truth_tokens = ground_truth.split()
        common = set(pred_tokens) & set(truth_tokens)

        if not common:
            return 0.0

        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(truth_tokens)
        f1 = 2 * (precision * recall) / (precision + recall)
        logger.info(f"F1 è®¡ç®—ç»“æœ: {f1:.4f}")
        return f1

    def execute(self, qa: List[Dict]) -> List[Dict]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        logger.info(f"å¼€å§‹å¤„ç† {len(qa)} æ¡æ•°æ®")
        results = []
        for item in qa:
            question = item.get("question")
            prompt = self.get_final_prompt()
            answer = self.query_llm(prompt, question)
            results.append({"question": question, "answer": answer})
        return results

    def evaluate(self):
        """è®¡ç®—F1åˆ†æ•°"""
        logger.info("å¼€å§‹è¯„ä¼°")
        prompt = self.get_final_prompt()
        data = self.load_data()
        if not data:
            logger.error("æœªåŠ è½½åˆ°æ•°æ®ï¼Œè¯„ä¼°ç»ˆæ­¢ã€‚")
            return 0.0

        answers = self.execute(data)
        f1_scores = []
        for item, result in zip(data, answers):
            question = item.get("question")
            ground_truth = item.get("answer")
            prediction = result.get("answer")
            if not question or not ground_truth:
                logger.warning("æ ·æœ¬æ•°æ®ç¼ºå°‘ question æˆ– answer å­—æ®µï¼Œè·³è¿‡è¯¥æ ·æœ¬ã€‚")
                continue
            f1 = self.compute_f1(prediction, ground_truth)
            f1_scores.append(f1)

        avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
        logger.info(f"ğŸ“Š æ•°æ®é›†å¹³å‡ F1 åˆ†æ•°: {avg_f1:.4f}")
        return avg_f1

def parse_args():
    parser = argparse.ArgumentParser(description="SPO PromptOptimizer CLI")
    parser.add_argument("--uid", type=str, default="3991ad42-c46b-4f2f-9dde-de015aaf5bde", help="ä¼˜åŒ–è¾“å‡ºè·¯å¾„çš„ UID")
    parser.add_argument("--name", type=str, default="Navigate", help="é¡¹ç›®åç§°")
    parser.add_argument("--model-url", type=str, default="https://api.chatanywhere.com.cn/v1", help="LLM æ¨¡å‹æ¥å£åœ°å€")
    parser.add_argument("--api-key", type=str, default="sk-iX0M9keAJemCgNFqvQMVLyWkcembRT27ix50aymLnvZ18QuT", help="çº¿ä¸Š API çš„å¯†é’¥")
    parser.add_argument("--dataset-name", type=str, default="bigbench", help="æ•°æ®é›†åç§°")
    parser.add_argument("--dataset-path", type=str, default="dataset", help="æœ¬åœ°æ•°æ®é›†è·¯å¾„")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    evaluator = F1_Evaluator(
        optimized_path=str(Path("workspace") / args.uid / args.name),
        model_url=args.model_url,
        dataset_name=args.dataset_name,
        dataset_path=args.dataset_path,
        api_key=args.api_key,
    )
    evaluator.evaluate()
